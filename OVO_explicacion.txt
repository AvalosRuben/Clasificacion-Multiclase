================================================================================
EXPLICACIÓN DETALLADA DEL CÓDIGO ONE VS ONE (OVO.ipynb)
Clasificación Multiclase usando One-vs-One con Regresión Logística
================================================================================

================================================================================
CELDA 1: IMPORTACIONES
================================================================================

Línea 1: from sklearn.base import BaseEstimator, ClassifierMixin
-----------------------------------------------------------------
¿Qué hace?
    - Importa dos clases base de scikit-learn necesarias para crear 
      clasificadores personalizados.
    
Entradas: Ninguna (es una importación)
Salidas: Clases BaseEstimator y ClassifierMixin disponibles

BaseEstimator:
    - Proporciona métodos get_params() y set_params() para gestionar parámetros
    - Permite que el clasificador sea compatible con GridSearchCV y pipelines
    
ClassifierMixin:
    - Proporciona el método score() automáticamente
    - Marca la clase como un clasificador en scikit-learn

¿Qué pasa si lo cambio?
    - Si eliminas BaseEstimator: perderás compatibilidad con herramientas de 
      scikit-learn como GridSearchCV
    - Si eliminas ClassifierMixin: no tendrás el método score() automático


Línea 2: from sklearn.linear_model import LogisticRegression
-------------------------------------------------------------
¿Qué hace?
    - Importa el modelo de Regresión Logística de scikit-learn
    
Entradas: Ninguna (es una importación)
Salidas: Clase LogisticRegression disponible

LogisticRegression:
    - Clasificador binario basado en regresión logística
    - Se usa como modelo base para cada par de clases en OVO

¿Qué pasa si lo cambio?
    - Podrías usar otro clasificador: SVM, DecisionTree, etc.
    - Ejemplo: from sklearn.svm import SVC
    - Cambiaría el rendimiento y características del modelo


Línea 3: import numpy as np
----------------------------
¿Qué hace?
    - Importa la librería NumPy para operaciones numéricas y manejo de arrays
    
Entradas: Ninguna (es una importación)
Salidas: Librería numpy disponible como 'np'

Usos en el código:
    - np.unique(): encontrar clases únicas
    - np.where(): filtrar datos
    - np.zeros(): crear matriz de votos
    - np.bincount(): contar votos
    - np.argmax(): encontrar la clase ganadora

¿Qué pasa si lo cambio?
    - Es ESENCIAL, no se puede eliminar sin reescribir todo el código


Línea 4: from itertools import combinations
--------------------------------------------
¿Qué hace?
    - Importa la función combinations del módulo itertools
    
Entradas: Ninguna (es una importación)
Salidas: Función combinations disponible

combinations(iterable, r):
    - Genera todas las combinaciones de r elementos del iterable
    - Para clases [0, 1, 2] con r=2: (0,1), (0,2), (1,2)
    - Orden no importa, sin repetición

¿Qué pasa si lo cambio?
    - Si usas permutations(): tendrías pares duplicados (0,1) y (1,0)
    - Si lo eliminas: tendrías que escribir manualmente los bucles para crear pares


================================================================================
CELDA 2: CLASE OneVsOneClassifierCustom
================================================================================

Línea 5: class OneVsOneClassifierCustom(BaseEstimator, ClassifierMixin):
-------------------------------------------------------------------------
¿Qué hace?
    - Define una nueva clase que hereda de BaseEstimator y ClassifierMixin
    
Entradas: Ninguna (es una definición)
Salidas: Nueva clase OneVsOneClassifierCustom

Herencia múltiple:
    - BaseEstimator: compatibilidad con scikit-learn
    - ClassifierMixin: comportamiento de clasificador

¿Qué pasa si lo cambio?
    - Si eliminas las herencias: el clasificador funcionará pero perderás 
      compatibilidad con scikit-learn
    - Si cambias el nombre: deberás actualizar todas las referencias


Línea 6-9: def __init__(self, base_model=LogisticRegression):
--------------------------------------------------------------
¿Qué hace?
    - Constructor de la clase, se ejecuta al crear una instancia
    - Inicializa los atributos del clasificador
    
Entradas:
    - base_model: clase de clasificador a usar (por defecto LogisticRegression)
    
Salidas: 
    - Objeto OneVsOneClassifierCustom inicializado

Atributos creados:
    - self.base_model: almacena la CLASE del modelo (no una instancia)
    - self.models = []: lista vacía para guardar todos los modelos entrenados
    - self.class_pairs = []: lista vacía para guardar los pares de clases

¿Qué pasa si lo cambio?
    - Si cambias base_model=SVC: usarás Support Vector Classifier
    - Si eliminas base_model: necesitarás siempre pasarlo al crear el objeto
    - Si no inicializas models/class_pairs: causará errores en fit()


Línea 11-13: def fit(self, X, y):
----------------------------------
¿Qué hace?
    - Define el método de entrenamiento del clasificador
    
Entradas:
    - X: matriz de características (n_muestras, n_características)
    - y: vector de etiquetas (n_muestras,)
    
Salidas:
    - self: el objeto entrenado (permite encadenamiento de métodos)


Línea 14-15: self.models = [] / self.class_pairs = []
------------------------------------------------------
¿Qué hace?
    - Reinicia las listas para permitir re-entrenamiento
    
Entradas: Ninguna
Salidas: Listas vacías

¿Por qué reiniciar?
    - Si llamas fit() dos veces, elimina los modelos anteriores
    - Evita acumular modelos viejos

¿Qué pasa si lo cambio?
    - Si eliminas estas líneas: los modelos se acumularán en múltiples fit()
    - Causaría resultados incorrectos al predecir


Línea 17-18: classes = np.unique(y)
------------------------------------
¿Qué hace?
    - Encuentra todas las clases únicas en el conjunto de entrenamiento
    
Entradas:
    - y: vector de etiquetas [0, 1, 1, 2, 0, 2, ...]
    
Salidas:
    - classes: array con clases únicas ordenadas [0, 1, 2]

np.unique():
    - Elimina duplicados
    - Ordena los valores
    - Para Iris: classes = [0, 1, 2] (Setosa, Versicolor, Virginica)

¿Qué pasa si lo cambio?
    - Si usas set(y): obtendrías un conjunto, no un array NumPy
    - Si no ordenas: el orden de las clases sería impredecible


Línea 20-21: for (c1, c2) in combinations(classes, 2):
-------------------------------------------------------
¿Qué hace?
    - Itera sobre todos los pares posibles de clases
    
Entradas:
    - classes: [0, 1, 2]
    
Iteraciones:
    - Primera: c1=0, c2=1
    - Segunda: c1=0, c2=2
    - Tercera: c1=1, c2=2
    
Total de pares: C(n,2) = n*(n-1)/2
    - Para 3 clases: 3 pares
    - Para 4 clases: 6 pares
    - Para 10 clases: 45 pares

¿Qué pasa si lo cambio?
    - Si cambias a combinations(classes, 3): harías clasificadores de 3 clases
    - Si usas un bucle manual: sería más largo y propenso a errores


Línea 22-23: idx = np.where((y == c1) | (y == c2))
---------------------------------------------------
¿Qué hace?
    - Encuentra los índices donde la etiqueta es c1 O c2
    
Entradas:
    - y: vector completo de etiquetas
    - c1, c2: las dos clases del par actual
    
Salidas:
    - idx: tupla con array de índices que cumplen la condición

Ejemplo:
    - Si y = [0, 1, 2, 0, 1, 2] y c1=0, c2=1
    - idx = ([0, 1, 3, 4],)

Operador |:
    - OR lógico bit a bit
    - (y == c1): [True, False, False, True, False, False]
    - (y == c2): [False, True, False, False, True, False]
    - Resultado: [True, True, False, True, True, False]

¿Qué pasa si lo cambio?
    - Si usas & (AND): no obtendrías ninguna muestra (imposible ser c1 Y c2)
    - Si olvidas los paréntesis: error de precedencia de operadores


Línea 24-25: X_pair = X[idx] / y_pair = y[idx]
-----------------------------------------------
¿Qué hace?
    - Filtra los datos para incluir solo las muestras de las dos clases actuales
    
Entradas:
    - X: matriz completa de características
    - y: vector completo de etiquetas
    - idx: índices a extraer
    
Salidas:
    - X_pair: subconjunto de X con solo muestras de c1 y c2
    - y_pair: subconjunto de y con solo etiquetas c1 y c2

Ejemplo:
    - Si tenemos 150 muestras en Iris
    - X_pair podría tener ~100 muestras (solo Setosa y Versicolor)
    - y_pair tendría solo valores 0 y 1

¿Qué pasa si lo cambio?
    - Si no filtras: entrenarías con todas las clases (no sería One-vs-One)
    - Si solo filtras X pero no y: error de dimensiones


Línea 27-29: model = self.base_model() / model.fit(X_pair, y_pair)
-------------------------------------------------------------------
¿Qué hace?
    - Crea una NUEVA INSTANCIA del modelo base
    - Entrena el modelo solo con el par de clases actual
    
Entradas:
    - X_pair: características del par de clases
    - y_pair: etiquetas del par de clases
    
Salidas:
    - model: modelo entrenado para distinguir entre c1 y c2

self.base_model():
    - Los paréntesis () crean una instancia
    - self.base_model es la CLASE (LogisticRegression)
    - model es una INSTANCIA de esa clase

model.fit():
    - Entrena el modelo binario
    - Solo aprende a distinguir c1 de c2

¿Qué pasa si lo cambio?
    - Si reutilizas el mismo modelo: sobrescribirías el entrenamiento anterior
    - Si no llamas fit(): el modelo no estaría entrenado


Línea 31-32: self.models.append(model) / self.class_pairs.append((c1, c2))
---------------------------------------------------------------------------
¿Qué hace?
    - Guarda el modelo entrenado y el par de clases correspondiente
    
Entradas:
    - model: modelo recién entrenado
    - (c1, c2): tupla con el par de clases
    
Salidas:
    - Listas actualizadas

Estado final (para Iris):
    - self.models: [modelo_0vs1, modelo_0vs2, modelo_1vs2]
    - self.class_pairs: [(0,1), (0,2), (1,2)]

Índices correspondientes:
    - models[0] clasifica class_pairs[0]
    - models[1] clasifica class_pairs[1]
    - etc.

¿Qué pasa si lo cambio?
    - Si no guardas los pares: no sabrías qué modelo clasifica qué
    - Si el orden no coincide: predicciones incorrectas


Línea 34: return self
----------------------
¿Qué hace?
    - Devuelve el objeto entrenado
    
Entradas: Ninguna
Salidas: self (el objeto completo)

Propósito:
    - Permite encadenamiento: ovo = OneVsOneClassifierCustom().fit(X, y)
    - Convención de scikit-learn

¿Qué pasa si lo cambio?
    - Si devuelves None: no podrás encadenar métodos
    - Si devuelves otra cosa: romperías la convención de scikit-learn


Línea 36-37: def predict(self, X):
-----------------------------------
¿Qué hace?
    - Define el método de predicción del clasificador
    
Entradas:
    - X: matriz de características a predecir (n_muestras, n_características)
    
Salidas:
    - Array de predicciones (n_muestras,)


Línea 38: votes = np.zeros((X.shape[0], len(self.class_pairs)))
----------------------------------------------------------------
¿Qué hace?
    - Crea una matriz para almacenar los votos de cada modelo
    
Entradas:
    - X.shape[0]: número de muestras a predecir
    - len(self.class_pairs): número de pares de clases (modelos)
    
Salidas:
    - votes: matriz de ceros

Dimensiones:
    - Filas: número de muestras (ej: 45 en test de Iris)
    - Columnas: número de modelos (ej: 3 para Iris)

Ejemplo para 2 muestras e Iris:
    votes = [[0, 0, 0],
             [0, 0, 0]]

¿Qué pasa si lo cambio?
    - Si usas otra dimensión: error de indexación posterior
    - Si no inicializas: tendrías valores aleatorios


Línea 40-41: predictions = []
------------------------------
¿Qué hace?
    - Inicializa una lista vacía (NO SE USA en el código actual)
    
Entradas: Ninguna
Salidas: Lista vacía

NOTA IMPORTANTE:
    - Esta línea es redundante en el código actual
    - No se usa en ninguna parte del método predict
    - Probablemente es código residual de una versión anterior

¿Qué pasa si lo cambio?
    - Si la eliminas: NO afecta el funcionamiento (es código muerto)


Línea 42-44: for i, model in enumerate(self.models):
-----------------------------------------------------
¿Qué hace?
    - Itera sobre todos los modelos entrenados con sus índices
    
Entradas:
    - self.models: lista de modelos entrenados
    
Iteraciones:
    - i=0, model=modelo_0vs1
    - i=1, model=modelo_0vs2
    - i=2, model=modelo_1vs2

enumerate():
    - Proporciona índice y valor simultáneamente
    - Alternativa sin enumerate: for i in range(len(self.models)): model = self.models[i]

¿Qué pasa si lo cambio?
    - Si no usas enumerate: tendrías que acceder manualmente con índices
    - Si no iteras sobre todos: perderías votos de algunos modelos


Línea 43: preds = model.predict(X)
-----------------------------------
¿Qué hace?
    - Obtiene las predicciones del modelo actual para todas las muestras
    
Entradas:
    - X: matriz completa de muestras a predecir
    - model: modelo entrenado para un par específico
    
Salidas:
    - preds: array de predicciones (valores: c1 o c2 del par)

Ejemplo:
    - Si model clasifica (0 vs 1)
    - preds podría ser [0, 1, 0, 1, 1, ...] para cada muestra

¿Qué pasa si lo cambio?
    - Si usas predict_proba(): obtendrías probabilidades en lugar de clases
    - Si no predices: votes quedaría con ceros


Línea 44: votes[:, i] = preds
------------------------------
¿Qué hace?
    - Almacena las predicciones del modelo i en la columna i de votes
    
Entradas:
    - preds: predicciones del modelo actual
    - i: índice del modelo
    
Salidas:
    - Matriz votes actualizada

votes[:, i] significa:
    - : = todas las filas
    - i = columna i-ésima

Ejemplo después de 3 iteraciones:
    votes = [[0, 0, 1],    # Muestra 1: modelo0→0, modelo1→0, modelo2→1
             [1, 2, 1],    # Muestra 2: modelo0→1, modelo1→2, modelo2→1
             ...]

¿Qué pasa si lo cambio?
    - Si usas votes[i, :]: transpondrías la matriz (error)
    - Si usas append: tendrías una lista, no array


Línea 46-50: # Voto mayoritario por fila
-----------------------------------------
¿Qué hace?
    - Determina la clase ganadora para cada muestra mediante voto mayoritario
    
Entradas:
    - votes: matriz con todos los votos
    
Salidas:
    - final_preds: lista con la clase predicha para cada muestra


Línea 47: final_preds = []
---------------------------
¿Qué hace?
    - Inicializa lista para almacenar predicciones finales
    
Entradas: Ninguna
Salidas: Lista vacía

¿Qué pasa si lo cambio?
    - Si no inicializas: error al hacer append


Línea 48: for row in votes:
----------------------------
¿Qué hace?
    - Itera sobre cada fila de votes (cada fila = una muestra)
    
Entradas:
    - votes: matriz de votos
    
Iteraciones:
    - Primera: row = [0, 0, 1] (votos para muestra 1)
    - Segunda: row = [1, 2, 1] (votos para muestra 2)
    - etc.

¿Qué pasa si lo cambio?
    - Si iteras sobre columnas: calcularías votos por modelo (incorrecto)


Línea 49: counts = np.bincount(row.astype(int))
------------------------------------------------
¿Qué hace?
    - Cuenta cuántas veces aparece cada clase en los votos de esa muestra
    
Entradas:
    - row: votos de una muestra [0, 0, 1]
    
Salidas:
    - counts: array donde counts[i] = número de votos para clase i

Ejemplo:
    - row = [0, 0, 1]
    - counts = [2, 1] → clase 0 tuvo 2 votos, clase 1 tuvo 1 voto

np.bincount():
    - Cuenta enteros no negativos
    - Índice del array = valor contado
    - Valor en ese índice = número de ocurrencias

row.astype(int):
    - Convierte a enteros (por si acaso son floats)

¿Qué pasa si lo cambio?
    - Si no conviertes a int: posible error con bincount
    - Si usas Counter de collections: funcionaría pero es menos eficiente


Línea 50: final_preds.append(np.argmax(counts))
------------------------------------------------
¿Qué hace?
    - Encuentra la clase con más votos y la añade a las predicciones finales
    
Entradas:
    - counts: array de conteo de votos
    
Salidas:
    - final_preds actualizado

np.argmax(counts):
    - Devuelve el ÍNDICE del valor máximo
    - El índice corresponde a la clase

Ejemplo:
    - counts = [2, 1, 0] → clase 0 tiene más votos
    - np.argmax(counts) = 0
    - Se predice clase 0

Desempates:
    - Si hay empate, argmax elige la primera clase (menor índice)

¿Qué pasa si lo cambio?
    - Si usas np.max(): obtendrías el número de votos, no la clase
    - Si usas otro criterio: cambiarías la estrategia de decisión


Línea 52: return np.array(final_preds)
---------------------------------------
¿Qué hace?
    - Convierte la lista de predicciones a array NumPy y la devuelve
    
Entradas:
    - final_preds: lista de predicciones [0, 2, 1, 1, ...]
    
Salidas:
    - Array NumPy de predicciones

np.array():
    - Convierte lista a array NumPy
    - Formato estándar de scikit-learn

¿Qué pasa si lo cambio?
    - Si devuelves la lista: funcionará pero no es el estándar
    - Si no devuelves nada: predicciones serían None


================================================================================
CELDA 3: IMPORTACIONES DE UTILIDADES
================================================================================

Línea 53: from sklearn.datasets import load_iris
-------------------------------------------------
¿Qué hace?
    - Importa la función para cargar el dataset Iris
    
Entradas: Ninguna (es una importación)
Salidas: Función load_iris disponible

Dataset Iris:
    - 150 muestras, 3 clases (50 por clase)
    - 4 características: longitud/ancho de sépalo y pétalo
    - Clases: Setosa (0), Versicolor (1), Virginica (2)

¿Qué pasa si lo cambio?
    - Puedes usar load_wine, load_digits, etc.
    - Cambiaría el número de clases y características


Línea 54: from sklearn.model_selection import train_test_split
---------------------------------------------------------------
¿Qué hace?
    - Importa función para dividir datos en entrenamiento y prueba
    
Entradas: Ninguna (es una importación)
Salidas: Función train_test_split disponible

train_test_split:
    - Divide datos aleatoriamente
    - Mantiene proporciones de clases (con stratify)
    - Esencial para evaluar correctamente

¿Qué pasa si lo cambio?
    - Si no divides: sobreajuste (evalúas con datos de entrenamiento)
    - Podrías usar KFold para validación cruzada


Línea 55: from sklearn.metrics import accuracy_score, confusion_matrix
-----------------------------------------------------------------------
¿Qué hace?
    - Importa métricas para evaluar el modelo
    
Entradas: Ninguna (es una importación)
Salidas: Funciones accuracy_score y confusion_matrix disponibles

accuracy_score:
    - Calcula % de predicciones correctas
    - Métrica simple pero útil

confusion_matrix:
    - Matriz n×n donde n = número de clases
    - Fila i, columna j = muestras reales de i predichas como j
    - Diagonal = predicciones correctas

¿Qué pasa si lo cambio?
    - Podrías agregar precision_score, recall_score, f1_score
    - Diferentes métricas para diferentes necesidades


================================================================================
CELDA 4: CARGA Y ENTRENAMIENTO
================================================================================

Línea 57-58: data = load_iris() / X, y = data.data, data.target
----------------------------------------------------------------
¿Qué hace?
    - Carga el dataset Iris y separa características de etiquetas
    
Entradas: Ninguna
Salidas:
    - data: objeto Bunch con todo el dataset
    - X: matriz (150, 4) con características
    - y: array (150,) con etiquetas

Estructura:
    - X[0] = [5.1, 3.5, 1.4, 0.2] (primera muestra)
    - y[0] = 0 (Setosa)

¿Qué pasa si lo cambio?
    - Si usas otro dataset: cambian dimensiones y número de clases
    - Si no separas X e y: no podrás entrenar


Línea 60-63: X_train, X_test, y_train, y_test = train_test_split(...)
----------------------------------------------------------------------
¿Qué hace?
    - Divide los datos en conjuntos de entrenamiento (70%) y prueba (30%)
    
Entradas:
    - X, y: datos completos
    - test_size=0.3: 30% para prueba
    - random_state=42: semilla para reproducibilidad
    
Salidas:
    - X_train: (105, 4) características de entrenamiento
    - X_test: (45, 4) características de prueba
    - y_train: (105,) etiquetas de entrenamiento
    - y_test: (45,) etiquetas de prueba

test_size=0.3:
    - 30% de 150 = 45 muestras para test
    - 70% de 150 = 105 muestras para train

random_state=42:
    - Fija la semilla aleatoria
    - Resultados reproducibles

¿Qué pasa si lo cambio?
    - test_size=0.5: más datos de prueba, menos de entrenamiento
    - sin random_state: resultados diferentes cada vez
    - test_size muy pequeño: evaluación poco confiable
    - test_size muy grande: poco entrenamiento


Línea 65-66: ovo = OneVsOneClassifierCustom() / ovo.fit(X_train, y_train)
--------------------------------------------------------------------------
¿Qué hace?
    - Crea una instancia del clasificador OVO y la entrena
    
Entradas:
    - X_train: características de entrenamiento
    - y_train: etiquetas de entrenamiento
    
Salidas:
    - ovo: objeto OneVsOneClassifierCustom entrenado

Proceso interno:
    1. Crea 3 modelos (uno por par de clases)
    2. Entrena cada modelo con su subconjunto de datos
    3. Guarda modelos y pares

¿Qué pasa si lo cambio?
    - Si pasas base_model=SVC: usarías SVM en lugar de Regresión Logística
    - Si no llamas fit: el modelo no estaría entrenado


Línea 68-69: y_pred = ovo.predict(X_test)
------------------------------------------
¿Qué hace?
    - Predice las clases para el conjunto de prueba
    
Entradas:
    - X_test: (45, 4) características de prueba
    
Salidas:
    - y_pred: (45,) predicciones de clase

Proceso interno:
    1. Cada modelo predice para las 45 muestras
    2. Se recopilan votos en matriz 45×3
    3. Se cuenta voto mayoritario
    4. Se devuelve clase ganadora para cada muestra

¿Qué pasa si lo cambio?
    - Si predices sobre X_train: evaluarías entrenamiento (no correcto)
    - Si el modelo no está entrenado: error


================================================================================
CELDA 5: VISUALIZACIÓN DE RESULTADOS
================================================================================

Línea 70: import matplotlib.pyplot as plt
------------------------------------------
¿Qué hace?
    - Importa la librería de gráficos matplotlib
    
Entradas: Ninguna (es una importación)
Salidas: Módulo pyplot disponible como plt

matplotlib.pyplot:
    - Interfaz tipo MATLAB para crear gráficos
    - plt.figure(), plt.plot(), plt.imshow(), etc.

¿Qué pasa si lo cambio?
    - Podrías usar seaborn para gráficos más estéticos
    - Sin importar: no podrás visualizar


Línea 72-73: cm = confusion_matrix(y_test, y_pred)
---------------------------------------------------
¿Qué hace?
    - Calcula la matriz de confusión comparando valores reales vs predichos
    
Entradas:
    - y_test: (45,) etiquetas reales
    - y_pred: (45,) predicciones del modelo
    
Salidas:
    - cm: matriz (3, 3) con conteos

Estructura de cm:
                Predicción
              0    1    2
    Real  0  [15   0    0]    <- Todas las Setosa bien clasificadas
          1  [ 0  13    2]    <- 13 Versicolor correctas, 2 como Virginica
          2  [ 0   1   14]    <- 1 Virginica como Versicolor, 14 correctas

Interpretación:
    - Diagonal: predicciones correctas
    - Fuera de diagonal: errores
    - Suma de cada fila: total de muestras de esa clase

¿Qué pasa si lo cambio?
    - Si inviertes el orden: transpones la matriz (interpretación diferente)
    - Si agregas normalize='true': obtienes proporciones en lugar de conteos


Línea 75: plt.figure()
-----------------------
¿Qué hace?
    - Crea una nueva figura de matplotlib
    
Entradas: Ninguna (usa valores por defecto)
Salidas: Objeto Figure

¿Para qué?
    - Prepara el lienzo para graficar
    - Puedes especificar tamaño: plt.figure(figsize=(10, 8))

¿Qué pasa si lo cambio?
    - Si no creas figura: matplotlib crea una automáticamente
    - Si especificas figsize: cambias dimensiones del gráfico


Línea 76: plt.imshow(cm)
-------------------------
¿Qué hace?
    - Muestra la matriz de confusión como una imagen de colores
    
Entradas:
    - cm: matriz de confusión (3, 3)
    
Salidas:
    - Gráfico de matriz de colores

plt.imshow():
    - Trata cada valor como intensidad de color
    - Valores altos = colores claros/brillantes
    - Valores bajos = colores oscuros

Mapa de colores por defecto:
    - viridis: azul (bajo) a amarillo (alto)

¿Qué pasa si lo cambio?
    - plt.imshow(cm, cmap='Blues'): colores azules
    - plt.imshow(cm, cmap='hot'): negro-rojo-amarillo
    - Sin imshow: tendrías que usar otro tipo de gráfico


Línea 77: plt.title("Matriz de Confusión - One vs One (Logistic Regression)")
------------------------------------------------------------------------------
¿Qué hace?
    - Añade un título al gráfico
    
Entradas:
    - String con el título
    
Salidas:
    - Título visible en el gráfico

¿Qué pasa si lo cambio?
    - Puedes personalizar: fontsize, color, position
    - Si no pones título: gráfico sin título (menos informativo)


Línea 78-79: plt.xlabel("Predicción") / plt.ylabel("Real")
-----------------------------------------------------------
¿Qué hace?
    - Añade etiquetas a los ejes X e Y
    
Entradas:
    - Strings con las etiquetas
    
Salidas:
    - Etiquetas visibles en los ejes

Convención:
    - Eje X: predicciones del modelo
    - Eje Y: valores reales

¿Qué pasa si lo cambio?
    - Si inviertes: confundirías la interpretación
    - Si no pones etiquetas: menos claridad


Línea 81-82: clases = ["Setosa", "Versicolor", "Virginica"]
------------------------------------------------------------
¿Qué hace?
    - Define una lista con los nombres de las clases de Iris
    
Entradas: Ninguna
Salidas:
    - clases: lista de strings

Orden:
    - Índice 0: "Setosa" (clase 0)
    - Índice 1: "Versicolor" (clase 1)
    - Índice 2: "Virginica" (clase 2)

¿Qué pasa si lo cambio?
    - Si cambias el orden: etiquetas no corresponderán con los datos
    - Si usas otro dataset: necesitarías otros nombres


Línea 83: plt.xticks(np.arange(len(clases)), clases, rotation=45)
------------------------------------------------------------------
¿Qué hace?
    - Coloca etiquetas de texto en el eje X
    
Entradas:
    - np.arange(len(clases)): [0, 1, 2] (posiciones)
    - clases: ["Setosa", "Versicolor", "Virginica"] (etiquetas)
    - rotation=45: rotación de 45 grados
    
Salidas:
    - Etiquetas visibles en eje X

np.arange(len(clases)):
    - len(clases) = 3
    - np.arange(3) = [0, 1, 2]

rotation=45:
    - Evita solapamiento de texto
    - Mejor legibilidad

¿Qué pasa si lo cambio?
    - rotation=90: texto vertical
    - rotation=0: texto horizontal (puede solaparse)
    - Sin etiquetas personalizadas: verías solo números


Línea 84: plt.yticks(np.arange(len(clases)), clases)
-----------------------------------------------------
¿Qué hace?
    - Coloca etiquetas de texto en el eje Y
    
Entradas:
    - np.arange(len(clases)): [0, 1, 2] (posiciones)
    - clases: ["Setosa", "Versicolor", "Virginica"] (etiquetas)
    
Salidas:
    - Etiquetas visibles en eje Y

Sin rotation:
    - El texto en Y suele caber sin rotación

¿Qué pasa si lo cambio?
    - Puedes añadir rotation si es necesario
    - Sin etiquetas: solo números


Línea 86-89: for i in range(cm.shape[0]): / for j in range(cm.shape[1]):
--------------------------------------------------------------------------
¿Qué hace?
    - Bucle anidado para recorrer todas las celdas de la matriz de confusión
    
Entradas:
    - cm.shape[0]: número de filas (3)
    - cm.shape[1]: número de columnas (3)
    
Iteraciones:
    - i=0, j=0: celda (0,0)
    - i=0, j=1: celda (0,1)
    - i=0, j=2: celda (0,2)
    - i=1, j=0: celda (1,0)
    - ... (9 iteraciones totales)

cm.shape:
    - Para matriz 3×3: shape = (3, 3)
    - shape[0] = filas
    - shape[1] = columnas

¿Qué pasa si lo cambio?
    - Si usas valores fijos (range(3)): menos flexible
    - Si solo iteras i: solo una dimensión


Línea 88: plt.text(j, i, cm[i, j], ha='center', va='center')
-------------------------------------------------------------
¿Qué hace?
    - Coloca el valor numérico en cada celda de la matriz
    
Entradas:
    - j: posición X (columna)
    - i: posición Y (fila)
    - cm[i, j]: valor a mostrar
    - ha='center': alineación horizontal centrada
    - va='center': alineación vertical centrada
    
Salidas:
    - Texto visible en cada celda

Ejemplo:
    - Para celda (0,0): muestra cm[0,0] = 15 en posición (0,0)
    - Para celda (1,2): muestra cm[1,2] = 2 en posición (2,1)

ha y va:
    - ha = horizontal alignment
    - va = vertical alignment
    - Opciones: 'left', 'center', 'right', 'top', 'bottom'

¿Qué pasa si lo cambio?
    - Si inviertes (i, j): texto en posiciones incorrectas
    - Si quitas ha/va: texto desalineado
    - Si añades color/fontsize: personalizas apariencia


Línea 90: plt.show()
---------------------
¿Qué hace?
    - Muestra el gráfico en pantalla
    
Entradas: Ninguna
Salidas: Ventana con el gráfico

¿Cuándo es necesario?
    - En scripts: necesario para mostrar
    - En notebooks: opcional (se muestra automáticamente)

¿Qué pasa si lo cambio?
    - Si lo omites en script: no verías el gráfico
    - Si lo omites en notebook: funciona igual
    - plt.savefig('imagen.png'): guarda en lugar de mostrar


================================================================================
RESUMEN DE FLUJO COMPLETO
================================================================================

1. IMPORTACIONES:
   - Herramientas de scikit-learn para crear clasificadores
   - NumPy para operaciones numéricas
   - itertools para generar pares de clases

2. CLASE OneVsOneClassifierCustom:
   - Constructor: inicializa modelo base y listas vacías
   - fit(): entrena un modelo para cada par de clases
   - predict(): voto mayoritario entre todos los modelos

3. CARGA DE DATOS:
   - Dataset Iris: 150 muestras, 3 clases, 4 características
   - División 70/30 en entrenamiento/prueba

4. ENTRENAMIENTO:
   - Se crean 3 modelos (C(3,2) = 3 pares)
   - Cada modelo clasifica 2 de las 3 clases

5. PREDICCIÓN:
   - Cada modelo vota
   - Se cuenta el voto mayoritario
   - Se asigna la clase más votada

6. EVALUACIÓN:
   - Matriz de confusión
   - Visualización con matplotlib
   - Valores numéricos en cada celda


================================================================================
VENTAJAS Y DESVENTAJAS DE ONE-VS-ONE
================================================================================

VENTAJAS:
✓ Cada modelo es binario (más simple)
✓ Datasets balanceados en cada modelo (solo 2 clases)
✓ Funciona bien con SVM y otros clasificadores binarios
✓ Menor impacto de clases desbalanceadas

DESVENTAJAS:
✗ Número de modelos crece cuadráticamente: C(n,2) = n(n-1)/2
✗ Para 10 clases: 45 modelos
✗ Para 100 clases: 4,950 modelos
✗ Predicción más lenta (todos los modelos deben votar)
✗ Empates posibles en el voto


================================================================================
PARÁMETROS CONFIGURABLES Y SUS EFECTOS
================================================================================

1. base_model:
   - LogisticRegression (por defecto)
   - SVC: más preciso pero más lento
   - DecisionTreeClassifier: más rápido pero posible sobreajuste
   - RandomForestClassifier: mejor generalización

2. test_size:
   - 0.3 (30%): balance estándar
   - 0.2 (20%): más datos de entrenamiento
   - 0.5 (50%): validación más robusta

3. random_state:
   - 42 (fijo): resultados reproducibles
   - None: diferentes resultados cada vez
   - Otro número: otra división específica

4. Mapa de colores (cmap):
   - 'viridis': por defecto
   - 'Blues': tonos azules
   - 'Reds': tonos rojos
   - 'hot': negro-rojo-amarillo


================================================================================
FIN DE LA EXPLICACIÓN
================================================================================
